{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "https://docs.google.com/presentation/d/1v54Tr4POZj9K4zsaHOn7U22rzdSlTXsyCgE5lfhd-rA/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use San Antonio's 311 call data for this lesson and exercises. Dowload the CSV data from Google classroom\n",
    "\n",
    "https://classroom.google.com/u/0/w/Mzg3MTg5NzU1Njk1/tc/Mzg3MTg5NzU1NzE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in CSV file \n",
    "source = (spark.read.csv(\"source.csv\",\n",
    "                     sep=\",\",\n",
    "                     header=True,\n",
    "                     inferSchema=True)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to read in data:\n",
    "\n",
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Schemas\n",
    "Spark includes a concept of a data schema  \n",
    "Specify the types of our data ahead of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Read csv, but now we specify the schema:\n",
    "\n",
    "source = spark.read.csv(\"source.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to a destination using .write property\n",
    "\n",
    "source.write.json(\"source_json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the case.csv file\n",
    "\n",
    "df = spark.read.csv(\"case.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataframe\n",
    "len(df.columns), df.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first three records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#datatypes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to do:**\n",
    "\n",
    "1. **Rename Columns:**\n",
    "    - 'SLA_due_date -> case_due_date\n",
    "\n",
    "\n",
    "\n",
    "2. **Correct Data Types:**\n",
    "    - case_closed and case_late to boolean\n",
    "    - council_district as a string\n",
    "    - case_opened_date, case_closed_date and case_due_date to datetime format\n",
    "\n",
    "\n",
    "3. **Data Transformation:**\n",
    "    - request_address: trim and lowercase\n",
    "    - format council district with leading zeros\n",
    "    - convert the number of days a case is late to a number of weeks\n",
    "    \n",
    "    \n",
    "4. **New features:**\n",
    "    - zip_code : extract from address\n",
    "    - case_age \n",
    "    - days_to_closed\n",
    "    - case_lifetime\n",
    "    \n",
    "    \n",
    "5. **Join cases data with department data:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename Columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename 'SLA_due_date' to 'case_due_date' using .withColumnRenamed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct Data Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct data types: case_closed and case_late to boolean\n",
    "\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .withColumn to change columns from string to boolean values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the columns\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# council_district cast as string\n",
    "df.select('council_district').show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# council_district as a string instead of int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert case_opened_date, case_closed_date and case_due_date to datetime format\n",
    "\n",
    "df.select('case_opened_date', 'case_closed_date', 'case_due_date').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_timestamp, fmt\n",
    "\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the three columns again\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request_address: trim and lowercase\n",
    "\n",
    "df.select('request_address').show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request_address: trim and lowercase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the number of days a case is late to a number of weeks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use format_string function to pad zeros for council_district\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column for zipcode:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case_age: How old the case is; the difference in days between when the case was opened and the current day  \n",
    "days_to_closed: The number of days between when the case was opened and when it was closed  \n",
    "case_lifetime: Number of days between when the case was opened and when it was closed, if the case is still open, the number of days since the case was opened  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create three new columns 'case_age', 'days_to_closed', 'case_lifetime'\n",
    "\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dept.csv file:\n",
    "\n",
    "dept = spark.read.csv(\"dept.csv\", header=True, inferSchema=True)\n",
    "dept.show(5, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dept.select('dept_division').distinct().show()\n",
    "# dept.select('dept_name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the df and dept dataframe using 'dept_division' as common key\n",
    "# drop columns as needed (keep standardized_dept_name)\n",
    "# convert dept_subject_to_SLA to boolean\n",
    "\n",
    "df = (\n",
    "    df\n",
    "    # left join on dept_division\n",
    "    .join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(1, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
