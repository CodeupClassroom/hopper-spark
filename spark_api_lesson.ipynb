{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0bef1af",
   "metadata": {},
   "source": [
    "# Spark 101\n",
    "\n",
    "In this lesson we will cover the basics of working with spark dataframes, and show how spark dataframes are different from the pandas dataframes we have been working with.\n",
    "\n",
    "While spark dataframes might superficially look like pandas dataframes, and even share some of the same methods and syntax, it is important to keep in mind they are 2 seperate types of objects, and, while spark and pandas code might look superficially similar, it tends to be semantically very different.\n",
    "\n",
    "We'll begin by creating the spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa5826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e52308",
   "metadata": {},
   "source": [
    "## Creating Dataframes\n",
    "\n",
    "Spark can convert any pandas dataframe into a spark dataframe with a simple method call. For this lesson, we will use this functionality to demonstrate the differences between spark and pandas dataframes and explore how to work with spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ffa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20))\n",
    ")\n",
    "pandas_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c08e242",
   "metadata": {},
   "source": [
    "Here we start with a simple pandas dataset, and now we will convert it to a spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4403fea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1191a4ed",
   "metadata": {},
   "source": [
    "Notice that, while we do see the column names, we don't see the data in the dataframe like we would with a pandas dataframe. This is because spark is lazy, in that it won't show us values until it has to. For the purposes of looking at the first few rows of our data, we can use the `.show` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681eb54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228d0c9",
   "metadata": {},
   "source": [
    "Like pandas dataframes, spark dataframes have a `.describe` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eecf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49509fac",
   "metadata": {},
   "source": [
    "Which, also like pandas, returns another dataframe. However, since this is a spark dataframe, we have to explicitly show it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fb360",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775518c",
   "metadata": {},
   "source": [
    "By default spark will show the first 20 rows, but we can specify how many we want by passing a number to `.show`.\n",
    "\n",
    "Observing the shape of our spark dataframe does require a little bit of creativity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4486029",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DataFrame shape: \", df.count(), \" x \", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa86a943",
   "metadata": {},
   "source": [
    "Let's use some different data so that we have a more robust dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb44ae",
   "metadata": {},
   "source": [
    "Let's look at another difference from pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c224fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8cb241",
   "metadata": {},
   "source": [
    "While this expression would produce a Series of values from a pandas dataframe, for a spark dataframe this produces a Column object, which is an object that represents a vertical slice of a dataframe, but does not contain the data itself.\n",
    "\n",
    "One way to use our column objects is to use them in combination with the `.select` method. `.select` is very powerful, and lets us specify what data we want to see in the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd43c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6578e54c",
   "metadata": {},
   "source": [
    "Again, notice that we don't see any data, instead we see the new dataframe that is produced. To see the actual data, we'll again need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64f588",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5673be59",
   "metadata": {},
   "source": [
    "Our column objects support a number of operations, including the arithmetic operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa79fb",
   "metadata": {},
   "source": [
    "Here we get back a column that represents the values from the original hwy column with 1 added to them. To actually see this data, we'd need to select it and show the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca02e978",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy, mpg.hwy + 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aa60c5",
   "metadata": {},
   "source": [
    "Once we have a column object, we can use the `.alias` method to rename it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy.alias(\"highway_mileage\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d7a78",
   "metadata": {},
   "source": [
    "We can also store column objects in variables and reference them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = mpg.hwy.alias(\"highway_mileage\")\n",
    "col2 = (mpg.hwy / 2).alias(\"highway_mileage_halved\")\n",
    "mpg.select(col1, col2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbac13",
   "metadata": {},
   "source": [
    "## Other ways to create columns\n",
    "\n",
    "In addition to the syntax we've seen above, we can create columns with the `col` and `expr` functions from `pyspark.sql.functions` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394c7d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c74c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9874c91",
   "metadata": {},
   "source": [
    "The output above is somewhat meaningless. Without attaching this col to a source, it's just a lazy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col = col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a84c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc51a4e",
   "metadata": {},
   "source": [
    "We can mix and match the syntax we use, and the column object produced by the `col` function is the same as the the previous column object we saw."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b1299",
   "metadata": {},
   "source": [
    "Here `col(\"hwy\")` finds a match in the mpg data when we use `.select()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2da42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_column = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "\n",
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"highway_mileage\"),\n",
    "    mpg.cty.alias(\"city_mileage\"),\n",
    "    avg_column.alias(\"avg_mileage\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78e4fe",
   "metadata": {},
   "source": [
    "We created a variable named `avg_column` that represents the average of the highway and city mileage of each vehicle. This variable is created by using the `col` function to produce pyspark Column objects and using the arithmetic operators to combine them.\n",
    "\n",
    "Next we select the original highway and city mileage columns, in addition to our new average mileage column. We demonstrate the `col` function to select the `hwy` column and refer to the city mileage column with the `mpg.cty` syntax we saw previously. We also give all of our columns more readable aliases before showing the resulting dataframe.\n",
    "\n",
    "Notice something curious when we display `avg_column` by itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a7bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5f90b",
   "metadata": {},
   "source": [
    "It's like a recipe or formula for spark to use when it has something to match `hwy` and `cty` to. Again, Spark is **lazy** and tries to limit the amount of information stored in memory. It accomplishes this by storing the instructions and only deploying those instructions when absolutely necessary.\n",
    "\n",
    "## `expr`\n",
    "The `expr` function is more powerful than `col`. It does everything `col` does and more. `expr` returns the same type of column object, but allows us to express manipulations to the column within the string that defines the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d350671",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    expr(\"hwy\"),  # the same as `col`\n",
    "    expr(\"hwy + 1\"),  # an arithmetic expression\n",
    "    expr(\"hwy AS highway_mileage\"),  # using an alias\n",
    "    expr(\"hwy + 1 AS highway_incremented\"),  # a combination of the above\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e83754a",
   "metadata": {},
   "source": [
    "There's a lot of redundancy in Spark..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff5d1d",
   "metadata": {},
   "source": [
    "Note that all the columns created above are identical, and which syntax to use is merely a style choice.\n",
    "\n",
    "## Type Casting\n",
    "\n",
    "We can view the types of the column in our dataframe in one of two ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a1802",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1756beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06204a34",
   "metadata": {},
   "source": [
    "Both provide the same information.\n",
    "\n",
    "To convert from one type to another, we can use the `.cast` method on a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy.cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5252f8c",
   "metadata": {},
   "source": [
    "Note that if a value is not able to be converted, it will be replaced with null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c572e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.model, mpg.model.cast(\"int\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11de902",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------\n",
    "## Exercise 1:\n",
    "\n",
    "Within your `codeup-data-science` directory, create a new repo named `spark-exercises`. This will be where you do your work for this module. Create a repository on GitHub with the same name, and link your local repository to GitHub.\n",
    "\n",
    "Save this work in your `spark-exercises` repo. Then add, commit, and push your changes.\n",
    "\n",
    "Create a jupyter notebook or python script named `spark101` for this exercise.\n",
    "\n",
    "Create a spark data frame that contains your favorite programming languages.\n",
    "\n",
    "- Create a dataframe with one column named `language`\n",
    "> Hint: Start with a pandas dataframe. Maybe use a dictionary?\n",
    "- View the schema of the dataframe\n",
    "- Output the shape of the dataframe\n",
    "- Show the first 5 records in the dataframe\n",
    "\n",
    "# ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec70307d",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "As we've seen through the column definitions, spark is very flexible and allows us many different ways to express ourselves. Another way that is fairly different than what we've seen above is through spark SQL, which lets us write SQL queries against our spark dataframes.\n",
    "\n",
    "In order to start using spark SQL, we'll first \"register\" the table with spark:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42756c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView(\"mpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bcf911",
   "metadata": {},
   "source": [
    "Now we can write a sql query against the `mpg` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1af08",
   "metadata": {},
   "source": [
    "Notice that the resulting value is another dataframe. As we know, in order to view the values in a dataframe, we need to use `.show`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3a8954",
   "metadata": {},
   "source": [
    "It is worth noting that all of the methods for creating / manipulating dataframes outlined above are the same in terms of performance as well. All of the resulting dataframes get turned into the same spark code that gets executed on the JVM, so it really is just a style choice as to which to use.\n",
    "\n",
    "## Basic Built-in Functions\n",
    "\n",
    "We've used the `col` and `expr` functions, but there are many other functions within the `pyspark.sql.functions` module, all of which operate on pyspark dataframe columns. Here we'll demonstrate several:\n",
    "\n",
    "- `concat`: to concatenate strings\n",
    "- `sum`: to sum a group\n",
    "- `avg`: to take the average of a group\n",
    "- `min`: to find the minimum\n",
    "- `max`: to find the maximum\n",
    "\n",
    "### WARNING:\n",
    "**Note that importing the `sum` function directly will override python's built-in `sum` function.** \n",
    "\n",
    "This means you will get an error if you try to `sum` a list of numbers, because `sum` will refernce the **pyspark** `sum` function, which works with pyspark dataframe columns, while the built-in `sum` function works with lists of numbers. **The same holds true for the built in `min` and `max` functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6359460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd9dc6",
   "metadata": {},
   "source": [
    "In this lesson we will explicitly import any functions from pyspark.sql.functions that we use, but it very common to see something like:\n",
    "\n",
    "> `from pyspark.sql.functions import *`\n",
    "\n",
    "which will import *all* of the functions from the `pyspark.sql.functions` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361eb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    sum(mpg.hwy) / count(mpg.hwy).alias(\"average_1\"),\n",
    "    avg(mpg.hwy).alias(\"average_2\"),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cb6b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b97e6",
   "metadata": {},
   "source": [
    "### `concat`\n",
    "\n",
    "We can create custom values by concatenating columns together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdabf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7b189",
   "metadata": {},
   "source": [
    "What if we want to concatenate a string that's not already in our data? In order to use a string literal as part of our select, we'll need to use the `lit` function, otherwise spark will try to resolve our string as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45917f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29582339",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(concat(mpg.cyl, lit(\" cylinders\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f64a9f",
   "metadata": {},
   "source": [
    "In the example above, we select the concatenation of the number of cylinders (the value from the `cyl` column) and the string literal \" cylinders\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we don't use `lit()`?\n",
    "mpg.select(concat(mpg.cyl, \" cylinders\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca7dbf4",
   "metadata": {},
   "source": [
    "## More `pyspark` Functions for String Manipulation\n",
    "\n",
    "Let's take a look at a couple more functions for string manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2adb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756c806",
   "metadata": {},
   "source": [
    "In order to demonstrate these functions we'll create a dataframe with some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d3b67",
   "metadata": {},
   "source": [
    "The `regexp_extract` function lets us specify a regular expression with at least one capture group, and create a new column based on the contents of a capture group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0963462",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_extract(\"address\", r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "    regexp_extract(\"address\", r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151d5c9",
   "metadata": {},
   "source": [
    "In the example above, the first argument to `regexp_extract` is the name of the string column to extract from, the second argument is the regular expression itself, and the last argument specifies which capture group we want to use. If, for example, our regular expression had 2 capture groups in it and we wanted the contents of the 2nd group, we would specify a 2 here.\n",
    "\n",
    "In addition to `regexp_extract`, `regexp_replace` lets us make substitutions based on a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c34ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf9c7e",
   "metadata": {},
   "source": [
    "In our example above, we obtain just the city, state, and zip code of the address by replacing everything up to the first comma with an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0c581",
   "metadata": {},
   "source": [
    "## `.filter` and `.where`\n",
    "\n",
    "Spark provides two dataframe methods, `.filter` and `.where`, which both allow us to select a subset of the rows of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc97dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.filter(mpg.cyl == 4).where(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c134f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.filter(mpg.cyl == 4).filter(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.where(mpg.cyl == 4).where(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5826e",
   "metadata": {},
   "source": [
    "They give the same result. There is no difference. \"filter\" is the standard Scala name for such a function and \"where\" is more familiar to people who use SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd96c7d2",
   "metadata": {},
   "source": [
    "## `when()` and `.otherwise()`\n",
    "\n",
    "Similar to:\n",
    "- an `IF` in Excel,\n",
    "- `CASE...WHEN` in SQL, \n",
    "- `np.where` in python, \n",
    "\n",
    "Spark provides a `when()` function.\n",
    "\n",
    "The `when` function lets us specify a condition, and **a value to produce** if that condition is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c319a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c4495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.hwy, when(mpg.hwy > 25, \"good_mileage\").alias(\"mpg_desc\")).show(\n",
    "    12\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee243df",
   "metadata": {},
   "source": [
    "Notice here that if the condition we specified is false, `null` will be produced. Instead of null, we can specify a value to use if our condition is false with the `.otherwise` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2521839f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    mpg.hwy,\n",
    "    when(mpg.hwy > 25, \"good_mileage\")\n",
    "    .otherwise(\"bad_mileage\")\n",
    "    .alias(\"mpg_desc\"),\n",
    ").show(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750103a",
   "metadata": {},
   "source": [
    "To specify multiple conditions, we can chain `.when` calls. The first condition that is met will be the value that is used, and if none of the conditions are met the value specified in the `.otherwise` will be used (or `null` if you don't provide a `.otherwise`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (\n",
    "        when(mpg.displ < 2, \"small\")\n",
    "        .when(mpg.displ < 3, \"medium\")\n",
    "        .otherwise(\"large\")\n",
    "        .alias(\"engine_size\")\n",
    "    ),\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11603d20",
   "metadata": {},
   "source": [
    "Notice here that a car with a `displ` of 1.8 matches both conditions we specified, but `small` is produced because it is associated with the first matching condition. For any value between 2 and 3, `medium` will be produced, and anything larger than 3 will produce `large`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b8d9a0",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------\n",
    "## Exercise 2:\n",
    "\n",
    "Load the `mpg` dataset as a spark dataframe.\n",
    "\n",
    "a. Create 1 column of output that contains a message like the one below for each record:\n",
    "\n",
    "    The 1999 audi a4 has a 4 cylinder engine.\n",
    "\n",
    "> Hint: You will need to concatenate values that already exist in the data with string literals\n",
    "\n",
    "b. Transform the trans column so that it only contains either manual or auto.\n",
    "\n",
    "> Hint: Consider spark string methods and `when().otherwise()` chaining\n",
    "# ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ab28a",
   "metadata": {},
   "source": [
    "## `withColumn()`\n",
    "\n",
    "We can add a new column to our dataframe using the withColumn() function. The first argument is the name of the new column, and the second argument is the forumla or value that we want in our column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c7059a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.withColumn(\"is_car\", lit(\"yes\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9fdc4",
   "metadata": {},
   "source": [
    "We can also use `.withColumn()` to perform operations using multiple columns as input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.manufacturer, mpg.model,mpg.hwy, mpg.cty).withColumn(\"ratio\", col('hwy') / col('cty')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed791d",
   "metadata": {},
   "source": [
    "**NOTE:** If I use `.select()`, I must include the columns used to create the `.withColumn()` result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e102d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.manufacturer, mpg.model).withColumn(\"ratio\", col('hwy') / col('cty')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ef803",
   "metadata": {},
   "source": [
    "## Sorting and Ordering\n",
    "\n",
    "Spark lets us sort the rows in our dataframe by one or multiple columns with two methods: `.sort`, and `.orderBy`. `.sort` and `.orderBy` are aliases of each other and do the exact same thing. Like other methods we've seen, `.sort` takes in a Column object or a string that is the name of a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d20d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6866c3",
   "metadata": {},
   "source": [
    "By default, values are sorted in ascending order. To sort in descending order, we can use the `.desc` method on any Column object, or the `desc` function from `pyspark.sql.functions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a76227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the .desc() method\n",
    "mpg.sort(mpg.hwy.desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f16e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the col() function with .desc() method\n",
    "mpg.sort(col(\"hwy\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39013f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the desc() function\n",
    "mpg.sort(desc(\"hwy\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b3ee7",
   "metadata": {},
   "source": [
    "To specify sorting by multiple columns, we provide each column as a separate argument to `.sort`.\n",
    "\n",
    "Here we will first reverse alphabetically by the vehicle's class, then by the number of cylinders from lowest to highest, then by the vehicle's highway mileage, from greatest to smallest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.sort(desc(\"class\"), mpg.cyl.asc(), col(\"hwy\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d09b9",
   "metadata": {},
   "source": [
    "## Grouping and Aggregating\n",
    "\n",
    "To aggregate our data by group, we can use the `.groupBy` method. Like with `.select`, we can pass either Column objects or strings that are column names to `.groupBy`. All of the expressions below are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupBy(mpg.cyl)\n",
    "mpg.groupBy(col(\"cyl\"))\n",
    "mpg.groupBy(\"cyl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e01c29",
   "metadata": {},
   "source": [
    "Once the data is grouped, we need to specify an aggregation. We can use one of the aggregate functions we imported earlier (`sum`, `avg`, `min`, `max`, etc.) along with a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787cab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9633c",
   "metadata": {},
   "source": [
    "To group by multiple columns, pass each of the columns a a separate argument to `.groupBy` (Note that this is different from pandas, where we would need to pass a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab0c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupBy(\"cyl\", \"class\").agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cb9db",
   "metadata": {},
   "source": [
    "In addition to `.groupBy`, we can use `.rollup`, which will do the same aggregations, but will also include the overall total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17448970",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.rollup(\"cyl\").count().sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec65db35",
   "metadata": {},
   "source": [
    "Here the null value in `cyl` indicates the total count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fb2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.rollup(\"cyl\").agg(expr(\"avg(hwy)\")).sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e700821b",
   "metadata": {},
   "source": [
    "And in the example above, the null row represents the overall average highway mileage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743346c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.rollup(\"cyl\", \"class\").mean(\"hwy\").sort(col(\"cyl\"), col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e8faac",
   "metadata": {},
   "source": [
    "## Crosstabs and Pivot Tables\n",
    "\n",
    "In addition to groupby, spark provides a couple other ways to do aggregation. One of which is `.crosstab`. This is very similary to pandas `.crosstab` function, in that it calculates the number of occurances of each unique value from the two passed columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.crosstab(\"class\", \"cyl\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437b994",
   "metadata": {},
   "source": [
    "`.crosstab` simply does counts, if we want a different aggregation, we can use `.pivot`. For example, to find the average highway mileage for each combination of car class and number of cylinders, we could write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e6bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupby(\"class\").pivot(\"cyl\").mean(\"hwy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bf04a",
   "metadata": {},
   "source": [
    "Here the unique values from the column we group by will be the rows in the resulting dataframe, and the unique values from the column we pivot on will become the columns. The values in each cell will be equal to the aggregation we specified over the group of values defined by the intersection of the rows and the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34cb5f",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------\n",
    "## Exercise 3: \n",
    "\n",
    "Load the `tips` dataset as a spark dataframe.\n",
    "\n",
    "a. What percentage of observations are smokers?\n",
    "> Hint: `.groupBy()` and `.withColumn()` are useful functions here\n",
    "\n",
    "b. Create a column that contains the tip percentage\n",
    "> Hint: `.withColumn()` is useful here\n",
    "\n",
    "c. Calculate the average tip percentage for each combination of sex and smoker.\n",
    "> Hint: Chain additional functions off the answer to part b \n",
    "\n",
    "# ------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0e729",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Let's take a look at how spark handles missing data. First we'll create a dataframe that has a few missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b02612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49377105",
   "metadata": {},
   "source": [
    "Spark provides two main ways to deal with missing values:\n",
    "\n",
    "- `.fill`: to replace missing values with a specified value\n",
    "- `.drop`: to drop rows containing missing values\n",
    "Both methods are accessed through the `.na` property. We'll look at some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ffbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d2e5d",
   "metadata": {},
   "source": [
    "For both methods, we can specify that we only want to fill or drop values in a specific column with a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1022473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.fill(0, subset=\"x\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c29b45a",
   "metadata": {},
   "source": [
    "Notice that above the na values in the x column were filled with 0, but the na values in y were left alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a40d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.na.drop(subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e5606",
   "metadata": {},
   "source": [
    "In the example above, the rows that had an na value for the y column were dropped, but the rows with na values for only the x column are still present.\n",
    "\n",
    "## Explaining DataFrame Transformations\n",
    "\n",
    "The `.explain` method will show us how spark is thinking about our dataframe.\n",
    "\n",
    "For our basic example, we see that there is only a single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb1fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c207b52",
   "metadata": {},
   "source": [
    "When we add a select function, there is another step after \"Scan ExistingRDD\", a \"Project\" that contains the names of the columns we are looking for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9977342",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecf784",
   "metadata": {},
   "source": [
    "Here we are doing a more advanced select calculation, but this is still just a single step to spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(((mpg.cyl + mpg.hwy) / 2).alias(\"avg_mpg\")).explain()m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768b227",
   "metadata": {},
   "source": [
    "A filter is also a single step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b04049",
   "metadata": {},
   "source": [
    "Without reading ahead, do you think the execution plan for the two dataframes below will be the same or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99eaed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(\"cyl\", \"hwy\").filter(expr(\"cyl = 6\")).explain()\n",
    "mpg.filter(expr(\"cyl = 6\")).select(\"cyl\", \"hwy\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407c539",
   "metadata": {},
   "source": [
    "Notice that even though we specified the transformations (`.select` and `.filter`) in a different order, we end up with the same output when we call `.explain`. This is because spark will look at our dataframe and transform it into the most efficient representation possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c90143",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.selectExpr(\"cyl + 3 * 16 / 4 + 19 AS unused\", \"hwy\").select(\n",
    "    \"hwy\"\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40b3cb",
   "metadata": {},
   "source": [
    "Notice above that we had 2 separate select statements, but spark condensed this down to a single Project, as it is smart enough to realize that it doesn't actually need to do all the arithmetic we specified in the first select, since we arent using that value later on.\n",
    "\n",
    "Now our execution plan gets more complicated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e63e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.select(min(mpg.cyl)).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919efeee",
   "metadata": {},
   "source": [
    "This is because in steps prior, we were applying transformations that applied to each row individually. To calculate a minimum, we have to look at all the rows in the dataset to find the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.groupby(mpg.cyl).agg(min(mpg.hwy), max(mpg.hwy)).explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a7eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    mpg.select(col(\"cyl\"), expr(\"(cty + hwy) / 2 AS avg_mpg\"))\n",
    "    .filter(expr('class == \"compact\"'))\n",
    "    .groupby(\"cyl\")\n",
    "    .agg(min(\"avg_mpg\"), avg(\"avg_mpg\"), max(\"avg_mpg\"))\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60267f42",
   "metadata": {},
   "source": [
    "## More Dataframe Manipulation Examples\n",
    "\n",
    "Let's take a look at some more examples of working with spark dataframes. For these examples, we'll be working with a dataset of observations of the weather in seattle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c7af",
   "metadata": {},
   "source": [
    "Let's print out the number of rows and columns in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3661cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weather.count(), \"rows\", len(weather.columns), \"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8903c",
   "metadata": {},
   "source": [
    "Let's first find the dates where the data starts and stops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7841b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a370da",
   "metadata": {},
   "source": [
    "We used `.select` to select the minimum date and the maximum date. `.first` returns us the first row of our results, which consists of two value, and so can be unpacked into the min_date and max_date variables.\n",
    "\n",
    "Next we will combine the temp max and min columns into a single column, `temp_avg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.withColumn(\n",
    "    \"temp_avg\", expr(\"ROUND(temp_min + temp_max) / 2\")\n",
    ").drop(\"temp_max\", \"temp_min\")\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef10983",
   "metadata": {},
   "source": [
    "Now we will calculate the total amount of rainfall for each month. We'll do this by first creating a month column, then grouping by the month, and finally, aggregating by taking the sum of the precipitation. To do this we will need to use the `month` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d2e7b",
   "metadata": {},
   "source": [
    "The `.sort` at the end isn't necessary, but presents that data in a friendlier way.\n",
    "\n",
    "Let's now take a look at the average temperature for each type of weather in December 2013:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 12)\n",
    "    .filter(year(\"date\") == 2013)\n",
    "    .groupBy(\"weather\")\n",
    "    .agg(mean(\"temp_avg\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d26b8c",
   "metadata": {},
   "source": [
    "Here we first have a couple of `.filter` calls in order to restrict our data to December of 2013. We then group by the weather column, and lastly, aggregate by taking the average of our `temp_avg` column. The combination of group by and agg will calculate the average temperature for each unique value of the `weather` column.\n",
    "\n",
    "Let's now find out how many days had freezing temperatures in each month of 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84efbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"freezing_temps\").alias(\"no_of_days_with_freezing_temps\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363821f",
   "metadata": {},
   "source": [
    "One last example, let's calculate the average temperature for each quarter of each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f173467",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\"))\n",
    "    .withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"year\", \"quarter\")\n",
    "    .agg(mean(\"temp_avg\").alias(\"temp_avg\"))\n",
    "    .sort(\"year\", \"quarter\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168ad09",
   "metadata": {},
   "source": [
    "Here we created the `quarter` and `year` columns, then grouped by these two new columns, and took the average temperature as our aggregate. Lastly, we sorted by the year and quarter for presentation purposes.\n",
    "\n",
    "We could also use a pivot table like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b955efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\"))\n",
    "    .withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"quarter\")\n",
    "    .pivot(\"year\")\n",
    "    .agg(expr(\"ROUND(MEAN(temp_avg), 2) AS temp_avg\"))\n",
    "    .sort(\"quarter\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b88efb",
   "metadata": {},
   "source": [
    "Here instead of grouping by two columns, we grouped by the first column and pivoted on the other column.\n",
    "\n",
    "## Joins\n",
    "\n",
    "Like pandas and sql, spark has functionality that lets us combine two tabular datasets, known as a join.\n",
    "\n",
    "We'll start by creating some data that we can join together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79a7884",
   "metadata": {},
   "source": [
    "To join two dataframes together, we'll need to call the `.join` method on one of them and supply the other as an argument. In addition, we'll need to supply the condition on which we are joining. In our case, we are joining where the `role_id` column on the users table is equal to the `id` column on the roles table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.join(roles, on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52968131",
   "metadata": {},
   "source": [
    "By default, spark will perform an inner join, meaning that records from both dataframes will have a match with the other. We can also specify either a left or a right join, which will keep all of the records from either the left or right side, even if those records don't have a match with the other dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.join(roles, on=users.role_id == roles.id, how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.join(roles, on=users.role_id == roles.id, how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e06964",
   "metadata": {},
   "source": [
    "Notice that examples above have a duplicate `id` column. There are several ways we could go about dealing with this:\n",
    "\n",
    "- alias each dataframe + explicitly select columns after joining (this could also be implemented with spark SQL)\n",
    "- rename duplicated columns before merging\n",
    "- drop duplicated columns after the merge (`.drop(right.id)`)\n",
    "\n",
    "## Visualization (or Lack Therof)\n",
    "\n",
    "Spark does not provide a way to do visualization with their dataframes. To visualize data from spark, you should use the `.toPandas` method on a spark dataframe to convert it to a pandas dataframe, then visualize as you normally would.\n",
    "\n",
    "> Converting a spark dataframe to a pandas dataframe will pull all the data into memory, so make sure you have enough available memory to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1958468",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------\n",
    "## Exercise 4:\n",
    "\n",
    "Use the seattle weather dataset referenced in the lesson to answer the questions below.\n",
    "\n",
    "- Convert the temperatures to fahrenheit.\n",
    "- Which month has the most rain, on average?\n",
    "- Which year was the windiest?\n",
    "- What is the most frequent type of weather in January?\n",
    "- What is the average high and low temperature on sunny days in July in 2013 and 2014?\n",
    "- What percentage of days were rainy in q3 of 2015?\n",
    "- For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
